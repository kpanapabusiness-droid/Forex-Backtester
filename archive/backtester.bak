# =============================================
# backtester.py — v1.9.8
# =============================================
# - Opens ONLY from entry_signal (no fallback to c1_signal)
# - Continuation logic handled in signal_logic (this file focuses on execution)
# - TP1 partial, move to BE after TP1, TS = 1.5× ATR (using ENTRY ATR)
# - Trailing activates only after a 2× ATR move from entry (close-based)
# - Intrabar priority configurable: tp_first | sl_first | best | worst
# - Uses your utils: calculate_atr, pip_size, pip_value_per_lot, position_size,
#   summarize_results, calculate_equity_curve
# - Writes: results/trades.csv, results/summary.txt, results/equity_curve.csv
# =============================================

# walk_forward.py  — v1.9.8 dict-friendly
from __future__ import annotations

__version__ = "1.9.8"

from copy import deepcopy
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union

import math
import importlib
import pkgutil
import inspect
from os import PathLike
import pandas as pd
import numpy as np
import json

from utils import get_pip_size, price_to_pips, pips_to_price

from validators_config import load_and_validate_config
from validators_util import validate_contract
from backtester_helpers import coerce_entry_exit_signals, finalize_trade_row

from indicators_cache import (
    compute_data_hash,
    compute_params_hash,
    cache_key_parts,
    load_from_cache,
    save_to_cache,
)

from utils import (pip_value_per_lot,  # added by patch
    ensure_results_dir,
    load_pair_csv,
    write_summary,
    summarize_results,
    calculate_atr,
    TRADES_COLS,
    get_pip_size,
    price_to_pips,
    pips_to_price,
    pip_value_per_lot,
)
try:
    from signal_logic import apply_signal_logic  # ✅ required by simulate/run
except Exception as e:
    raise ImportError(
        "Could not import 'apply_signal_logic' from signal_logic.py. "
        "Ensure signal_logic.py exists in the project root and defines:\n"
        "    def apply_signal_logic(df: pd.DataFrame, config: dict) -> pd.DataFrame\n"
        f"Underlying import error: {e}"
    )

# =============================================
# Config / Paths
# =============================================

RESULTS_DIR_DEFAULT = Path("results")
TRADES_COLS = [
    "pair",
    "entry_date", "entry_price", "direction", "direction_int",
    "atr_at_entry_price", "atr_at_entry_pips",
    "lots_total", "lots_half", "lots_runner",

    # Risk / filters
    "risk_used", "dbcvix_flag",

    # --- Entry levels (immutable for audits) ---
    "tp1_price", "sl_price",
    "tp1_at_entry_price", "sl_at_entry_price",

    # --- State & TS ---
    "tp1_hit", "breakeven_after_tp1",
    "ts_active", "ts_level",

    # --- Exit info ---
    "entry_idx", "exit_date", "exit_price", "exit_reason",
    "sl_at_exit_price",

    # --- Results ---
    "pnl", "win", "loss", "scratch",
    "spread_pips_used", 'dbcvix_val', 'risk_pct_used']

# Ensure schema has the new fields
try:
    TRADES_COLS
except NameError:
    TRADES_COLS = []

if isinstance(TRADES_COLS, tuple):
    TRADES_COLS = list(TRADES_COLS)

for col in ("risk_pct_used", "dbcvix_val", "dbcvix_flag"):
    if col not in TRADES_COLS:
        TRADES_COLS.append(col)

# =============================================
# Helpers (no leading underscores)
# =============================================
# ---- DBCVIX resolver (config tolerant) ------------------------------------
from pathlib import Path
import pandas as _pd

def _first_non_none(*vals):
    for v in vals:
        if v is not None:
            return v
    return None

def _get_nested(d, *path, default=None):
    cur = d
    for key in path:
        if not isinstance(cur, dict) or key not in cur:
            return default
        cur = cur[key]
    return cur

def resolve_dbcvix_config(cfg: dict) -> dict:
    """
    Return a normalized DBCVIX config dict:
      {
        enabled: bool,
        mode: 'reduce'|'block',
        threshold: float,
        reduce_risk_to: float | None,
        source: 'csv'|'synthetic'|None,
        csv_path: str|None
      }
    Accepts multiple shapes to be future-proof:
      - cfg['risk_filters']['dbcvix']
      - cfg['filters']['dbcvix']
      - cfg['dbcvix']
      - cfg['rules']['risk_filters']['dbcvix']
      - cfg['risk']['dbcvix']  (legacy)
    Also tolerates any of: 'path' | 'file' | 'filepath' | 'csv_path'
    """
    candidates = [
        _get_nested(cfg, "risk_filters", "dbcvix"),
        _get_nested(cfg, "filters", "dbcvix"),
        _get_nested(cfg, "dbcvix"),
        _get_nested(cfg, "rules", "risk_filters", "dbcvix"),
        _get_nested(cfg, "risk", "dbcvix"),
    ]
    raw = next((c for c in candidates if isinstance(c, dict)), {}) or {}

    # unify path keys
    csv_path = raw.get("csv_path") or raw.get("path") or raw.get("file") or raw.get("filepath")
    mode = raw.get("mode", "reduce")
    threshold = raw.get("threshold", None)
    reduce_to = _first_non_none(raw.get("reduce_risk_to"), raw.get("target_risk"), raw.get("risk_to"))
    source = raw.get("source")
    enabled = bool(raw.get("enabled", False))

    # normalize types
    try:
        threshold = float(threshold) if threshold is not None else None
    except Exception:
        threshold = None
    try:
        reduce_to = float(reduce_to) if reduce_to is not None else None
    except Exception:
        reduce_to = None

    # Return normalized
    return {
        "enabled": enabled,
        "mode": mode if mode in ("reduce", "block") else "reduce",
        "threshold": threshold,
        "reduce_risk_to": reduce_to,
        "source": source if source in (None, "csv", "synthetic") else "csv" if csv_path else None,
        "csv_path": csv_path,
    }


def load_dbcvix_series(db_cfg: dict) -> _pd.Series | None:
    """
    Load the DBCVIX regime series if configured (CSV with columns: date, value).
    Returns a pandas Series indexed by date (pd.Timestamp) or None.
    """
    if not (db_cfg.get("enabled") and db_cfg.get("source") == "csv" and db_cfg.get("csv_path")):
        return None
    try:
        p = Path(db_cfg["csv_path"])
        df = _pd.read_csv(p)
        # tolerate a few common column names
        date_col = next((c for c in df.columns if c.lower() in ("date", "time", "timestamp")), None)
        val_col = next((c for c in df.columns if c.lower() in ("value", "dbcvix", "vix", "regime")), None)
        if not date_col or not val_col:
            raise ValueError("CSV must contain 'date' and 'value' columns (case-insensitive).")
        s = _pd.Series(df[val_col].astype(float).values, index=_pd.to_datetime(df[date_col]))
        s = s.sort_index()
        return s
    except Exception as e:
        print(f"⚠️  DBCVIX CSV load failed: {e}")
        return None

# --- DBCVIX loader ---
def load_dbcvix_series(cfg: dict) -> pd.Series | None:
    fcfg = ((cfg.get("filters") or {}).get("dbcvix") or {})
    if not fcfg.get("enabled"):
        return None

    src = fcfg.get("source", "synthetic")
    if src in ("synthetic", "manual_csv"):
        path = Path(fcfg.get("csv_path", "data/external/dbcvix_synth.csv"))
        col  = fcfg.get("column", "cvix_synth")
        if not path.exists():
            print(f"⚠️ DBCVIX CSV not found: {path}")
            return None
        s = pd.read_csv(path, parse_dates=["date"])
        if col not in s.columns:
            raise KeyError(f"DBCVIX column '{col}' not in {list(s.columns)}")
        ser = s.set_index("date")[col].astype(float).sort_index()
        return ser

    # Placeholders for terminal APIs if you add them later
    if src == "refinitiv":
        print("⚠️ Refinitiv source not implemented (RIC .DBCVIX).")
        return None
    if src == "bloomberg":
        print("⚠️ Bloomberg source not implemented (CVIX Index).")
        return None
    return None

# ===== Trailing-stop fill helpers (PATCH) =====================================

def _resolve_pip_size(pair: str) -> float:
    """
    Infer pip size by pair naming convention (e.g., 'GBP_JPY' -> 0.01 else 0.0001).
    Keeps existing behavior if you already have a util for this; harmless fallback.
    """
    pair = (pair or "").upper()
    return 0.01 if pair.endswith("JPY") else 0.0001

def _slippage_pips_from_cfg(cfg: dict) -> float:
    """
    Read a static slippage model from config if present. Defaults to 0 pips.
    You can later replace this with a richer model or a util hook without changing call sites.
    """
    fills = (cfg or {}).get("fills", {})
    sl  = fills.get("slippage", {})
    if not sl or not sl.get("enabled", False):
        return 0.0
    # fixed pips model {enabled: true, pips: 0.0}
    return float(sl.get("pips", 0.0))

def _apply_trailing_stop_fill(row: dict, *, final_stop_price: float, is_long: bool, pair: str, cfg: dict) -> dict:
    """
    Force exit at the stop (optionally ± slippage). Also stamps TS audit fields.
    Does NOT touch immutable entry fields (tp1_at_entry_price / sl_at_entry_price).
    """
    pip_size   = _resolve_pip_size(pair)
    slip_pips  = _slippage_pips_from_cfg(cfg)
    slip_px    = slip_pips * pip_size

    # Direction-aware slippage application
    if is_long:
        exit_px = final_stop_price - slip_px  # worse fill for longs
    else:
        exit_px = final_stop_price + slip_px  # worse fill for shorts

    # Write audited TS fields
    row["sl_at_exit_price"] = float(final_stop_price)  # price units
    row["ts_level"]         = float(final_stop_price)  # keep same level in price units
    row["ts_active"]        = True
    row["exit_price"]       = float(exit_px)
    row["slippage_pips"]    = float(slip_pips)

    # Keep canonical reason
    row["exit_reason"]      = "trailing_stop"
    return row
# ============================================================================


# =============================================
# Canonical DBCVIX resolver (ALWAYS returns 3-tuple)
# =============================================
# --- Fold generator (dates mode) ---
def _date_range_folds(start: "pd.Timestamp|str",
                      end: "pd.Timestamp|str",
                      train_years: int,
                      test_years: int,
                      step_years: int):
    """
    Yield rolling (is_start, is_end, oos_start, oos_end) windows.

    Rules:
      - Train and OOS windows are contiguous, no overlap:
          is_end < oos_start  and  oos_start = is_end + 1 day
      - Windows are inclusive on both ends.
      - We only yield full OOS windows that end <= global 'end'.
      - The fold start advances by 'step_years' each iteration.
    """
    import pandas as pd

    start = pd.to_datetime(start)
    end   = pd.to_datetime(end)

    cur = start
    one_day = pd.Timedelta(days=1)

    while True:
        is_start = cur
        is_end   = is_start + pd.DateOffset(years=int(train_years)) - one_day

        oos_start = is_end + one_day
        oos_end   = oos_start + pd.DateOffset(years=int(test_years)) - one_day

        # stop if OOS window would extend past global end
        if oos_end > end:
            break

        # safety: strict no-lookahead
        if not (is_end < oos_start):
            raise AssertionError(f"No-lookahead violation: train_end {is_end} !< oos_start {oos_start}")

        yield (is_start, is_end, oos_start, oos_end)

        # advance fold start
        cur = is_start + pd.DateOffset(years=int(step_years))
        if cur >= end:
            break


def _slice_df_by_dates(df: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:
    m = (df["date"] >= start) & (df["date"] <= end)
    out = df.loc[m].copy()
    return out.reset_index(drop=True)

def load_config(config_path: str | Path = "config.yaml") -> dict:
    """
    Load + validate YAML config.
    - Keeps your upward search for 'config.yaml'
    - Returns a normalized dict (with defaults) from Pydantic schema
    - Raises a readable ValueError if validation fails
    """
    path = Path(config_path)

    if not path.exists():
        # Walk up: CWD, parent, grandparent
        for root in (Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent):
            cand = root / "config.yaml"
            if cand.exists():
                path = cand
                break

    if not path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    # Delegate read + validation to validators_config
    return load_and_validate_config(str(path))

def load_raw_config(config_path: str | Path = "config.yaml") -> dict:
    path = Path(config_path)
    if not path.exists():
        for root in (Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent):
            cand = root / "config.yaml"
            if cand.exists():
                path = cand
                break
    if not path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

# =============================================
# Helper: Get the last value at/<= a timestamp
# =============================================
def ensure_results_dir(results_dir: Optional[PathLike], config: Optional[Dict[str, Any]] = None) -> Path:
    """
    Resolve and create the results directory.
    Priority:
      1) explicit results_dir arg
      2) config['output']['results_dir']
      3) default 'results'
    """
    if results_dir is None and config:
        results_dir = (config.get("output") or {}).get("results_dir")
    results_dir = results_dir or "results"
    out_dir = Path(results_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    return out_dir




def get_indicator_params(config: dict, candidates: list[str]) -> dict:
    """
    Pull params for an indicator from config['indicator_params'] using any of the candidate keys.
    Returns {} if nothing matches.
    """
    ip = (config.get("indicator_params") or {})
    for k in candidates:
        if k in ip and isinstance(ip[k], dict):
            return ip[k]
    return {}
    
# Resolve a confirmation function for C1/C2 from the shared pool.
# Tries c{role}_<name>, then falls back to c1_<name>.
def _resolve_confirmation_func(name: str, role: str = "c2"):
    mod = importlib.import_module("indicators.confirmation_funcs")
    for fn in (f"{role}_{name}", f"c1_{name}"):
        if hasattr(mod, fn):
            return getattr(mod, fn), fn  # (callable, resolved_name)
    raise AttributeError(f"No confirmation func found for role={role} name={name}")


# --- Summary writer (drop this into backtester.py) ---
# --- Summary writer (backtester.py) ---
def write_summary(path, trades_df, cfg, extra=None):
    """
    Writes a human-readable summary to `path`.
    Uses utils.summarize_results() if available; otherwise falls back to basic stats.
    """
    txt = None
    try:
        from utils import summarize_results
        txt = summarize_results(
            trades_df.to_dict("records"),
            starting_balance=float((cfg.get("risk") or {}).get("starting_balance",
                                   cfg.get("starting_balance", 10_000.0))),
            run_name=str(cfg.get("strategy_version", "default")),
            results_dir=str((cfg.get("output") or {}).get("results_dir", "results")),
            cfg=cfg,
        )
    except Exception:
        # Fallback basic summary
        total = int(len(trades_df))
        wins  = int(trades_df.get("win", pd.Series(False)).fillna(False).astype(bool).sum()) if total else 0
        losses = int(trades_df.get("loss", pd.Series(False)).fillna(False).astype(bool).sum()) if total else 0
        scratches = int(trades_df.get("scratch", pd.Series(False)).fillna(False).astype(bool).sum()) if total else 0
        non_scratch = max(wins + losses, 0)
        roi_dollars = float(pd.to_numeric(trades_df.get("pnl", 0.0), errors="coerce").fillna(0.0).sum())
        start_bal = float((cfg.get("risk") or {}).get("starting_balance",
                             cfg.get("starting_balance", 10_000.0)))
        roi_pct = (roi_dollars / start_bal * 100.0) if start_bal else 0.0

        lines = [
            "📊 Backtest Summary",
            "-------------------",
            f"Total Trades : {total}",
            f"Wins         : {wins}",
            f"Losses       : {losses}",
            f"Scratches    : {scratches}",
            f"Win% (NS)    : {((wins/non_scratch)*100.0) if non_scratch else 0.0:.2f}",
            f"ROI ($)      : {roi_dollars:.2f}",
            f"ROI (%)      : {roi_pct:.2f}",
        ]
        txt = "\n".join(lines)

    # Append any extra key/values (e.g., equity rows)
    if extra:
        extra_lines = []
        for k, v in extra.items():
            extra_lines.append(f"{k}: {v}")
        if extra_lines:
            sep = "\n" if txt and not txt.endswith("\n") else ""
            txt = f"{txt}{sep}" + "\n".join(extra_lines)

    path.write_text(txt or "", encoding="utf-8")



def resolve_spread_pips(pair: str, row: pd.Series, cfg: Dict[str, Any]) -> float:
    """
    Resolve spread (in pips) for PnL-only modeling.
    Priority:
      1) Per-bar column 'spread_pips' (if present)
      2) config.spreads.per_pair[pair]
      3) config.spreads.mode == 'atr_mult' -> k * atr_pips
      4) config.spreads.default_pips
    Returns 0.0 if spreads.enabled is False or nothing found.
    """
    sp = (cfg.get("spreads") or {})
    if not sp.get("enabled", False):
        return 0.0

    # per-bar override
    if "spread_pips" in row and pd.notna(row["spread_pips"]):
        try:
            return float(row["spread_pips"])
        except Exception:
            pass

    per_pair = (sp.get("per_pair") or {})
    if pair in per_pair:
        try:
            return float(per_pair[pair])
        except Exception:
            pass

    mode = str(sp.get("mode", "constant")).lower()
    if mode == "atr_mult":
        ps = pip_size_for_pair(pair)
        atr_val = float(row.get("atr", 0.0))
        atr_pips = (atr_val / ps) if ps else 0.0
        k = float(sp.get("atr_mult", 0.0))
        return float(atr_pips * k)

    try:
        return float(sp.get("default_pips", 0.0))
    except Exception:
        return 0.0
        
# === In-sim equity helpers (realized PnL only) ===
def _extract_bar_timestamp(row):
    # Prefer an explicit datetime-like column if you have one:
    # return pd.to_datetime(row["date"])
    # Fallback to index if your DF is time-indexed:
    return row.name



# ---- cache-aware indicator application (roles-gated) ----
from indicators_cache import (
    compute_data_hash,
    compute_params_hash,
    cache_key_parts,
    load_from_cache,
    save_to_cache,
)

# --- module search helpers ---
# --- indicator discovery imports (module-level) ---
from typing import Optional

# --- indicator module discovery helpers ---
def _iter_candidate_modules(role: str):
    base = "indicators"
    mapping = {
        "c1": [f"{base}.confirmation_funcs", f"{base}.c1_funcs"],
        "c2": [f"{base}.confirmation_funcs", f"{base}.c2_funcs"],
        "baseline": [f"{base}.baseline_funcs"],
        "volume": [f"{base}.volume_funcs"],
        "exit": [f"{base}.exit_funcs", f"{base}.exit_rules"],
    }
    mods = list(mapping.get(role, []))
    try:
        pkg = importlib.import_module(base)
        if hasattr(pkg, "__path__"):
            for m in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + "."):
                if m.name.endswith(("_funcs", "_rules", "_indicators")) and m.name not in mods:
                    mods.append(m.name)
    except Exception:
        pass
    return mods

def _resolve_indicator_func(role: str, name: Optional[str], verbose: bool):
    """Return (full_func_name_for_params, callable) or (None, None)."""
    if not name:
        return None, None
    wanted = str(name).lower()
    for modname in _iter_candidate_modules(role):
        try:
            mod = importlib.import_module(modname)
        except Exception:
            continue
        # exact & role-prefixed
        for cand in (name, f"{role}_{name}"):
            func = getattr(mod, cand, None)
            if callable(func):
                full = cand if cand.startswith(f"{role}_") else f"{role}_{name}"
                return full, func
        # fuzzy suffix match (e.g., yday ↔ c1_yday)
        for n, o in inspect.getmembers(mod, inspect.isfunction):
            low = n.lower()
            if low == wanted or low.endswith("_" + wanted):
                full = n if n.startswith(f"{role}_") else f"{role}_{n}"
                return full, o
    if verbose:
        print(f"⚠️ {role}/{name} not found in indicators/* modules")
    return None, None

def _call_indicator(func, frame: pd.DataFrame, params: dict, signal_col: str) -> pd.DataFrame:
    sig = inspect.signature(func)
    kwargs = {k: v for k, v in (params or {}).items() if k in sig.parameters}
    if "signal_col" in sig.parameters:
        kwargs["signal_col"] = signal_col
    return func(frame, **kwargs)

# --- cache-aware indicator application (roles-gated) ---
from indicators_cache import (
    compute_data_hash,
    compute_params_hash,
    cache_key_parts,
    load_from_cache,
    save_to_cache,
)


def apply_indicators_with_cache(df: pd.DataFrame, pair: str, cfg: dict) -> pd.DataFrame:
    """
    Applies indicators for roles (c1, c2, baseline, volume, exit), using cache.

    FIX: C2 confirmation pulls from the same pool as C1 (indicators.confirmation_funcs.c1_*).
    Resolution order for confirmations:
        1) <role>_<short>  (e.g., c2_coral)  [allows future role-specific funcs]
        2) c1_<short>      (shared pool; expected)
        3) <short>         (bare fallback)

    Caching keys remain role-scoped, so c2 usage has its own cache namespace even when reusing c1_* code.
    """
    import importlib

    cache_cfg = (cfg.get("cache") or {})
    cache_on  = cache_cfg.get("enabled", True)
    cache_dir = cache_cfg.get("dir", "cache")
    cache_fmt = cache_cfg.get("format", "parquet")
    scope_key = cache_cfg.get("scope_key")
    timeframe = cfg.get("timeframe", "D")
    verbose   = (cfg.get("tracking") or {}).get("verbose_logs", False)

    # roles allow-list (optional)
    roles_filter = set((cfg.get("cache") or {}).get("roles") or [])
    def role_enabled(role: str) -> bool:
        return not roles_filter or role in roles_filter

    inds = cfg.get("indicators") or {}
    def _get(k, default=None):
        # supports pydantic model or dict
        return getattr(inds, k, default) if hasattr(inds, k) else inds.get(k, default)

    data_hash = compute_data_hash(df)
    saves = hits = 0

    def _params_for(full_func_name: str) -> dict:
        return (cfg.get("indicator_params") or {}).get(full_func_name, {}) or {}

    # --- Shared confirmation resolver (C1 and C2 use the same pool) ---
    def _resolve_confirm_func(short_name: str, role: str = "c1"):
        """
        Load a confirmation function from indicators.confirmation_funcs.
        Returns (full_name, callable). Tries role-specific then c1_* then bare.
        """
        mod = importlib.import_module("indicators.confirmation_funcs")
        candidates = [f"{role}_{short_name}", f"c1_{short_name}", short_name]
        for name in candidates:
            if hasattr(mod, name):
                return f"indicators.confirmation_funcs.{name}", getattr(mod, name)
        raise ImportError(
            f"Function '{candidates[0]}' not found in module 'indicators.confirmation_funcs'. "
            f"Tried {candidates}."
        )

    def run_role(role: str, name: Optional[str], signal_col: str):
        nonlocal df, saves, hits
        if not name:
            return

        # Resolve function
        if role in ("c1", "c2"):
            try:
                full_name, func = _resolve_confirm_func(name, role=role)
            except Exception as e:
                if verbose:
                    print(f"❌ Confirm resolver failed for {role}/{name}: {e}")
                return
            params = _params_for(full_name)
        else:
            full_name, func = _resolve_indicator_func(role, name, verbose)
            if func is None:
                return
            params = _params_for(full_name)

        params_hash = compute_params_hash(params)
        parts_path, key = cache_key_parts(pair, timeframe, role, name, params_hash, data_hash, scope_key)

        # Try cache
        if cache_on:
            cached = load_from_cache(cache_dir, cache_fmt, parts_path, key)
            if cached is not None and not cached.empty:
                for col in cached.columns:
                    if col not in df.columns:
                        df[col] = cached[col]
                hits += 1
                if verbose:
                    print(f"⚡ Cache hit: {role}/{name}")
                return

        # Compute fresh
        before_cols = set(df.columns)
        df = _call_indicator(func, df, params, signal_col)
        created_cols = [c for c in (set(df.columns) - before_cols) if (c.endswith("_signal") or c == "baseline")]

        if cache_on and created_cols:
            save_to_cache(cache_dir, cache_fmt, parts_path, key, df[list(created_cols)].copy())
            saves += 1
            if verbose:
                print(f"📝 Cache save: {role}/{name} -> {parts_path}")

    # Run roles (gated)
    run_role("c1", _get("c1"), "c1_signal")
    if _get("use_c2", False) and role_enabled("c2"):
        run_role("c2", _get("c2"), "c2_signal")
    if _get("use_baseline", False) and role_enabled("baseline"):
        run_role("baseline", _get("baseline"), "baseline_signal")
    if _get("use_volume", False) and role_enabled("volume"):
        run_role("volume", _get("volume"), "volume_signal")
    if _get("use_exit", False) and role_enabled("exit"):
        run_role("exit", _get("exit"), "exit_signal")

    if verbose:
        print(f"📦 cache stats → saves={saves} hits={hits}")
    return df

def apply_indicators(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    inds = config.get("indicators", {}) or {}

    # Always ensure ATR first
    df = calculate_atr(df)

    # --- C1 (required) ---
    c1_name = inds.get("c1")
    if not c1_name:
        raise ValueError("Config indicators.c1 is missing.")
    mod = importlib.import_module("indicators.confirmation_funcs")
    c1_func = None
    for fname in [c1_name, f"c1_{c1_name}", f"confirmation_{c1_name}"]:
        if hasattr(mod, fname):
            c1_func = getattr(mod, fname); break
    if c1_func is None:
        raise ImportError(f"Could not find a C1 function for '{c1_name}' in indicators.confirmation_funcs")
    c1_params = get_indicator_params(config, [f"c1_{c1_name}", c1_name, getattr(c1_func, "__name__", "")])
    df = c1_func(df, signal_col="c1_signal", **c1_params)

    # --- C2 (optional; falls back to c1_<name>) ---
    if (inds.get("use_c2") and inds.get("c2")):            # <-- use inds
        c2_name = inds["c2"]                                # <-- use inds
        c2_func, resolved = _resolve_confirmation_func(c2_name, role="c2")

        ip = config.get("indicator_params", {}) or {}
        params = {}
        params.update(ip.get(f"indicators.confirmation_funcs.c2_{c2_name}", {}))
        params.update(ip.get(f"indicators.confirmation_funcs.{resolved}", {}))  # e.g., c1_lwpi
        params.update(ip.get(f"{resolved}", {}))                                # short-key fallback

        df = c2_func(df, signal_col="c2_signal", **params)
    else:
        df["c2_signal"] = 0  # safe default when disabled

    # --- Baseline (optional) ---
    if inds.get("use_baseline") and inds.get("baseline"):
        b_name = inds["baseline"]
        bmod = importlib.import_module("indicators.baseline_funcs")
        b_func = None
        for fname in [f"baseline_{b_name}", b_name]:
            if hasattr(bmod, fname):
                b_func = getattr(bmod, fname); break
        if b_func is None:
            raise ImportError(f"Could not find a Baseline function for '{b_name}' in indicators.baseline_funcs")
        b_params = get_indicator_params(config, [f"baseline_{b_name}", b_name, getattr(b_func, "__name__", "")])
        df = b_func(df, signal_col="baseline_signal", **b_params)
    else:
        if "baseline" not in df.columns:
            df["baseline"] = df["close"]
        df["baseline_signal"] = df.get("baseline_signal", 0)

    # --- Volume (optional) ---
    if inds.get("use_volume") and inds.get("volume"):
        v_name = inds["volume"]
        vmod = importlib.import_module("indicators.volume_funcs")
        v_func = None
        for fname in [f"volume_{v_name}", v_name]:
            if hasattr(vmod, fname):
                v_func = getattr(vmod, fname); break
        if v_func is None:
            raise ImportError(f"Could not find a Volume function for '{v_name}' in indicators.volume_funcs")
        v_params = get_indicator_params(config, [f"volume_{v_name}", v_name, getattr(v_func, "__name__", "")])
        df = v_func(df, signal_col="volume_signal", **v_params)
    else:
        df["volume_signal"] = df.get("volume_signal", 1)

    # --- Exit indicator (optional) ---
    if inds.get("use_exit") and inds.get("exit"):
        e_name = inds["exit"]
        emod = importlib.import_module("indicators.exit_funcs")
        e_func = None
        for fname in [f"exit_{e_name}", e_name]:
            if hasattr(emod, fname):
                e_func = getattr(emod, fname); break
        if e_func is None:
            raise ImportError(f"Could not find an Exit function for '{e_name}' in indicators.exit_funcs")
        e_params = get_indicator_params(config, [f"exit_{e_name}", e_name, getattr(e_func, "__name__", "")])
        df = e_func(df, signal_col="exit_signal", **e_params)
    else:
        df["exit_signal"] = 0   # <-- use 0, not None

    return df


def pip_size_for_pair(pair: str) -> float:
    # Use your utils implementation
    return float(get_pip_size(pair))

def pips_from_move(pair: str, move_in_price: float) -> float:
    return move_in_price / pip_size_for_pair(pair)

def price_from_pips(pair: str, pips: float) -> float:
    return pips * pip_size_for_pair(pair)

def load_pair_csv(pair: str, folder: str | Path) -> Optional[pd.DataFrame]:
    """
    Robust loader:
      - Handles USDCAD.csv, USD_CAD.csv, AUD_NZD_daily.csv, etc.
      - Recursively searches under data_dir
      - Normalizes columns and dtypes
    """
    folder = Path(folder)
    if not folder.exists():
        return None

    p_clean = pair.replace("/", "").replace("_", "").upper()   # USDCAD
    p_under = pair.replace("/", "_").upper()                   # USD_CAD
    variants = {
        p_under, p_under.lower(),
        p_clean, p_clean.lower(),
        pair.upper(), pair.lower(),
    }

    patterns = []
    for base in variants:
        patterns += [
            f"{base}.csv", f"{base}.CSV",
            f"{base}_D.csv", f"{base}_d.csv",
            f"{base}-D.csv", f"{base}-d.csv",
            f"{base}_daily.csv", f"{base}-daily.csv",
        ]

    matches: list[Path] = []
    for pat in patterns:
        matches += list(folder.rglob(pat))

    if not matches:
        # loose contains search as last resort
        tokens = {pair.replace("/", "").replace("_", ""), pair.replace("/", "_"), pair}
        toks = {t.upper() for t in tokens} | {t.lower() for t in tokens}
        for path in folder.rglob("*.csv"):
            if any(tok in path.name for tok in toks):
                matches.append(path)

    if not matches:
        return None

    matches = sorted(matches, key=lambda x: (len(str(x)), str(x)))
    df = pd.read_csv(matches[0])

    # normalize columns
    lower = {c.lower(): c for c in df.columns}
    if "date" in lower:
        df = df.rename(columns={lower["date"]: "date"})
    elif "time" in lower:
        df = df.rename(columns={lower["time"]: "date"})
    elif "datetime" in lower:
        df = df.rename(columns={lower["datetime"]: "date"})
    elif "timestamp" in lower:
        df = df.rename(columns={lower["timestamp"]: "date"})
    else:
        df = df.rename(columns={df.columns[0]: "date"})  # last resort

    rename_map = {}
    for want in ["open", "high", "low", "close", "volume"]:
        if want not in df.columns:
            for c in df.columns:
                if c.lower() == want:
                    rename_map[c] = want
                    break
    if rename_map:
        df = df.rename(columns=rename_map)

    missing = [c for c in ["date", "open", "high", "low", "close"] if c not in df.columns]
    if missing:
        raise ValueError(f"{matches[0].name} is missing required columns: {missing}")

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    for c in ["open", "high", "low", "close", "volume"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    df = df.dropna(subset=["date", "open", "high", "low", "close"]).sort_values("date").reset_index(drop=True)
    return df


def validate_signal_contract(df: pd.DataFrame) -> None:
    required = ["date", "open", "high", "low", "close", "pair", "entry_signal"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Signal logic missing required columns: {missing}")

def ensure_atr_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure 'atr' column exists in price units. Prefer your utils.calculate_atr().
    """
    if "atr" in df.columns:
        return df
    return calculate_atr(df)

def intrabar_sequence(priority: str) -> List[str]:
    """
    Returns ordered events ['tp','sl'] or ['sl','tp'] based on priority.
    """
    p = (priority or "tp_first").lower()
    if p == "sl_first": return ["sl", "tp"]
    if p == "best":     return ["tp", "sl"]   # favorable
    if p == "worst":    return ["sl", "tp"]   # unfavorable
    return ["tp", "sl"]                       # default

def project_trade_for_csv(tr: dict) -> dict:
    # Backfill for safety (legacy runs)
    if tr.get("tp1_at_entry_price") is None and tr.get("tp1_price") is not None:
        tr["tp1_at_entry_price"] = tr["tp1_price"]
    if tr.get("sl_at_entry_price") is None and tr.get("sl_price") is not None:
        tr["sl_at_entry_price"] = tr["sl_price"]
    if "sl_at_exit_price" not in tr:
        tr["sl_at_exit_price"] = tr.get(
            "current_sl",
            tr.get("sl_at_entry_price", tr.get("sl_price"))
        )

    # Only output declared columns (keeps CSV schema stable)
    return {col: tr.get(col) for col in TRADES_COLS}


# =============================================
# PnL with your pip valuation
# =============================================

def compute_trade_pnl_money(tr: Dict[str, Any], pair: str, pip_value_1lot: float) -> float:
    """
    PnL in account currency using pip valuation per 1 standard lot.
    PnL-only spread model:
      - Longs: entry at ASK (mid + sp/2), exits at BID (mid - sp/2)
      - Shorts: entry at BID (mid - sp/2), exits at ASK (mid + sp/2)
    We do NOT change trigger logic; this only affects realized cash PnL.
    """
    dir_int   = int(tr["direction_int"])
    entry_mid = float(tr["entry_price"])
    exit_mid  = float(tr["exit_price"])
    lots_half   = float(tr["lots_half"])
    lots_runner = float(tr["lots_runner"])
    tp1_hit   = bool(tr.get("tp1_hit", False))
    tp1_mid   = float(tr.get("tp1_price") or entry_mid)
    sp_pips   = float(tr.get("spread_pips_used", 0.0))

    ps = pip_size_for_pair(pair)
    sp_price = sp_pips * ps  # spread in price units

    # Fill prices for PnL (PnL-only model)
    if dir_int > 0:
        # LONG: buy at ASK, sell at BID
        entry_fill = entry_mid + sp_price / 2.0
        tp1_fill   = tp1_mid   - sp_price / 2.0
        exit_fill  = exit_mid  - sp_price / 2.0
    else:
        # SHORT: sell at BID, buy at ASK
        entry_fill = entry_mid - sp_price / 2.0
        tp1_fill   = tp1_mid   + sp_price / 2.0
        exit_fill  = exit_mid  + sp_price / 2.0

    def pips_between(px2, px1):
        # signed pips in direction of the trade
        return (dir_int * (px2 - px1)) / ps if ps else 0.0

    if tp1_hit:
        # Half closes at TP1; runner closes at final exit — both referenced to SAME entry fill
        pips_half   = pips_between(tp1_fill,  entry_fill)
        pips_runner = pips_between(exit_fill, entry_fill)
        pnl = (pips_half * pip_value_1lot * lots_half) + (pips_runner * pip_value_1lot * lots_runner)
    else:
        pips_full = pips_between(exit_fill, entry_fill)
        pnl = pips_full * pip_value_1lot * (lots_half + lots_runner)

    return float(pnl)

# backtester.py
def _finalize_and_append_trade(
    trades_list: list,
    trade_row: dict,
    *,
    current_sl: float | None,
) -> None:
    """
    Centralized finalize+append so all exit branches persist the stop in effect at exit.

    Rules:
      - breakeven_after_tp1  -> sl_at_exit = entry_price
      - trailing_stop        -> sl_at_exit = ts_level (if present) else current_sl
      - stoploss (non-TS)    -> sl_at_exit = current_sl
      - other exits          -> sl_at_exit unchanged (None), serializer will keep prior
    """
    exit_reason = str(trade_row.get("exit_reason", "")).lower()

    if "breakeven" in exit_reason:
        sl_at_exit = trade_row.get("entry_price")
    elif "trailing_stop" in exit_reason or "trailing" in exit_reason:
        sl_at_exit = trade_row.get("ts_level", current_sl)
    elif "stop" in exit_reason:
        sl_at_exit = current_sl
    else:
        sl_at_exit = None

    finalized = finalize_trade_row(
        dict(trade_row),  # shallow copy; safe if callers reuse dicts
        current_stop_price_at_exit=sl_at_exit,
    )
    trades_list.append(finalized)


# =============================================
# Core Simulation
# =============================================
def simulate_pair_trades(
    rows: pd.DataFrame,
    pair: str,
    cfg: Dict[str, Any],
    equity_state: Dict[str, float],
    return_equity: bool = False,
    **overrides,
) -> List[Dict[str, Any]] | tuple[List[Dict[str, Any]], pd.DataFrame]:
    """
    Simulates trades on a single pair using signals in `rows`.

    Implements:
      - Entry ONLY on `entry_signal` (+1 long / -1 short)
      - TP1 (half close) -> move SL to breakeven
      - Trailing stop activates AFTER a 2×ATR (entry ATR) move based on CLOSE
      - TS distance = 1.5×ATR (entry ATR), ratchets only in favorable direction
      - Intrabar priority: 'tp_first' | 'sl_first' | 'best' | 'worst'
      - Exit reasons + Win/Loss/Scratch classification per user rules
      - Equity updates applied on each exit

    Patch notes:
      - Keeps existing behavior.
      - Adds per-pair realized PnL cumulative series (realized_pnl_cum_local).
      - If return_equity=True, returns eq_df with ['date','pair','pnl_realized_cum','equity'].
      - Robustness: sanitize ATR, ensure math import, and skip zero-sized entries.
      - 🔧 Exit serialization hygiene: use finalize_trade_row(...) to persist the final stop at exit.
      - 🔧 TS EXIT FILL FIX: when exit_reason == 'trailing_stop', exit_price is set to the final stop (± optional slippage from cfg), and we stamp sl_at_exit_price/ts_level/ts_active.
    """
    # --- DBCVIX wiring (via **overrides) ---
    dbcvix_series = overrides.get("dbcvix_series")  # may be None
    dbcvix_cfg    = ((cfg.get("filters") or {}).get("dbcvix") or {})

    # ---- Config shorthands
    entry_cfg = (cfg.get("entry") or {})
    risk_cfg  = (cfg.get("risk") or {})
    exec_cfg  = (cfg.get("execution") or {})
    exit_cfg  = (cfg.get("exit") or {})

    SL_ATR_MULT       = float(overrides.get("sl_atr_mult",      entry_cfg.get("sl_atr", 1.5)))
    TP1_ATR_MULT      = float(overrides.get("tp1_atr_mult",     entry_cfg.get("tp1_atr", 1.0)))
    TRAIL_AFTER_ATR   = float(overrides.get("trail_after_atr",  entry_cfg.get("trail_after_atr", 2.0)))
    TS_ATR_MULT       = float(overrides.get("ts_atr_mult",      entry_cfg.get("ts_atr", 1.5)))
    intrabar_priority = str(overrides.get("intrabar_priority",  exec_cfg.get("intrabar_priority", "tp_first")))

    account_ccy = (risk_cfg.get("account_ccy") or "AUD").upper()
    risk_pct    = float(risk_cfg.get("risk_per_trade", 0.02))
    fx_quotes   = risk_cfg.get("fx_quotes") or {}

    # ---- Helpers
    ps = pip_size_for_pair(pair)
    def to_pips(price_move: float) -> float:
        return price_move / ps if ps else 0.0

    def _cfg_slippage_pips(cfg: Dict[str, Any]) -> float:
        """Read a fixed slippage model from cfg.fills.slippage; defaults to 0 pips if absent/disabled."""
        fills = (cfg or {}).get("fills") or {}
        sl    = fills.get("slippage") or {}
        if not sl or not sl.get("enabled", False):
            return 0.0
        try:
            return float(sl.get("pips", 0.0))
        except Exception:
            return 0.0

    def hit_level(direction_int: int, high_px: float, low_px: float, level: float, kind: str) -> bool:
        """Return True if intrabar hits `level` for a long/short and 'tp' or 'sl' check."""
        if level is None or not math.isfinite(level):
            return False
        if direction_int > 0:  # long
            if kind == "tp": return high_px >= level
            if kind == "sl": return low_px  <= level
        else:                  # short
            if kind == "tp": return low_px  <= level
            if kind == "sl": return high_px >= level
        return False

    def signed_move_from_entry(direction_int: int, px_now: float, px_entry: float) -> float:
        return direction_int * (px_now - px_entry)

    def trail_level_from_close(direction_int: int, close_px: float, atr_entry: float) -> float:
        """Compute candidate TS level using ENTRY ATR and current close."""
        if direction_int > 0:
            return close_px - TS_ATR_MULT * atr_entry
        else:
            return close_px + TS_ATR_MULT * atr_entry

    def better_stop(direction_int: int, a: Optional[float], b: Optional[float]) -> Optional[float]:
        """Pick the stop that is *tighter* in the correct direction (ratchet)."""
        if a is None: return b
        if b is None: return a
        return max(a, b) if direction_int > 0 else min(a, b)

    def _int_signal(val) -> int:
        """Coerce any scalar to {-1,0,1} safely (tolerates NaN/None/strings)."""
        try:
            v = float(val)
            if math.isnan(v):
                return 0
            v = int(v)
            if v > 1:  v = 1
            if v < -1: v = -1
            return v
        except Exception:
            return 0

    # ---- Outputs and state
    trades: List[Dict[str, Any]] = []
    open_tr: Optional[Dict[str, Any]] = None

    # Local realized PnL cumulative for THIS PAIR (does not change global equity directly)
    realized_pnl_cum_local: float = 0.0
    equity_history = []  # rows: {'date', 'pair', 'pnl_realized_cum', 'equity'}

    # ---- Intrabar event order
    order = intrabar_sequence(intrabar_priority)  # e.g., ['tp', 'sl']

    # Ensure time-sorted and clean index
    if "date" in rows.columns:
        rows = rows.sort_values("date").reset_index(drop=True)
    else:
        rows = rows.sort_index().reset_index(drop=True)

    for i in range(len(rows)):
        r = rows.iloc[i]
        date_i = pd.to_datetime(r["date"]) if "date" in r else pd.to_datetime(r.name)
        o_i, h_i, l_i, c_i = float(r["open"]), float(r["high"]), float(r["low"]), float(r["close"])

        # --- Sanitize ATR (defensive) ---
        atr_raw = pd.to_numeric(r.get("atr"), errors="coerce")
        atr_i = float(0.0 if pd.isna(atr_raw) else atr_raw)

        entry_sig = _int_signal(r.get("entry_signal", 0))
        exit_sig  = _int_signal(r.get("exit_signal", 0))

        # ===========================
        # 1) Manage existing position
        # ===========================
        if open_tr is not None:
            d = int(open_tr["direction_int"])
            entry_px   = float(open_tr["entry_price"])
            atr_entry  = float(open_tr["atr_at_entry_price"])
            tp1_px     = float(open_tr["tp1_price"])
            sl_px      = float(open_tr["sl_price"])
            tp1_done   = bool(open_tr.get("tp1_hit", False))
            ts_active  = bool(open_tr.get("ts_active", False))
            ts_level   = open_tr.get("ts_level", None)
            be_price   = entry_px if tp1_done else None  # once TP1 hits, SL moves to BE

            # (a) Activate trailing once CLOSE move >= TRAIL_AFTER_ATR * ATR_entry
            if not ts_active and math.isfinite(atr_entry) and atr_entry > 0:
                move_cl = signed_move_from_entry(d, c_i, entry_px)
                if move_cl >= TRAIL_AFTER_ATR * atr_entry:
                    ts_active = True
                    ts_level = trail_level_from_close(d, c_i, atr_entry)

            # (b) If trailing is active, ratchet in our favor
            if ts_active and math.isfinite(atr_entry) and atr_entry > 0:
                cand = trail_level_from_close(d, c_i, atr_entry)
                ts_level = better_stop(d, ts_level, cand)

            # (c) Effective stop to check on this bar
            effective_stop = sl_px
            if tp1_done and be_price is not None:
                effective_stop = better_stop(d, effective_stop, be_price)
            if ts_active and ts_level is not None:
                effective_stop = better_stop(d, effective_stop, ts_level)

            # (d) Intrabar checks for TP1 and SL/TS/BE
            closed_this_bar = False
            reason = None
            exit_px = None

            # If TP1 not yet done, honor intrabar order between TP1 and SL
            if not tp1_done:
                for ev in order:
                    if ev == "tp" and hit_level(d, h_i, l_i, tp1_px, "tp"):
                        # Mark TP1: half exits at tp1_px; move SL to BE
                        tp1_done = True
                        be_price = entry_px
                        sl_px = be_price  # immediately move protective stop to BE
                        open_tr["tp1_hit"] = True
                        open_tr["breakeven_after_tp1"] = True
                        # keep runner open; continue checking SL in the same bar
                    elif ev == "sl" and hit_level(d, h_i, l_i, effective_stop, "sl"):
                        # SL hit before TP1 -> full loss
                        reason = "stoploss"
                        exit_px = effective_stop
                        closed_this_bar = True
                        break
                # After order loop, recompute effective stop if TP1 occurred
                if not closed_this_bar and tp1_done:
                    effective_stop = sl_px
                    if ts_active and ts_level is not None:
                        effective_stop = better_stop(d, effective_stop, ts_level)

            # If still open (either TP1 already done before, or done above), check BE/TS on same bar
            if (not closed_this_bar) and (tp1_done or ts_active):
                if hit_level(d, h_i, l_i, effective_stop, "sl"):
                    # Decide reason: trailing vs breakeven
                    if ts_active and ts_level is not None and (
                        (d > 0 and effective_stop == max(sl_px, ts_level, be_price or -1e18)) or
                        (d < 0 and effective_stop == min(sl_px, ts_level, be_price or  1e18))
                    ):
                        reason = "trailing_stop"
                    else:
                        reason = "breakeven_after_tp1" if tp1_done and abs(effective_stop - entry_px) < 1e-12 else "stoploss"
                    exit_px = effective_stop
                    closed_this_bar = True

            # Indicator/logic exits at CLOSE (only if still open and no stop was hit)
            if (not closed_this_bar) and exit_sig != 0:
                # Map to configured reason
                if exit_cfg.get("exit_on_exit_signal", False):
                    reason = "exit_indicator"
                elif exit_cfg.get("exit_on_c1_reversal", True):
                    reason = "c1_reversal"
                elif exit_cfg.get("exit_on_baseline_cross", False):
                    reason = "baseline_cross"
                else:
                    reason = "exit_indicator"
                exit_px = c_i
                closed_this_bar = True

            # If closing, finalize trade
            if closed_this_bar:
                # 🔧 ======= BEGIN: Trailing-stop exit fill fix =======
                if reason == "trailing_stop":
                    # Force fill at the final stop (± optional slippage), stamp TS audit fields
                    slip_pips = _cfg_slippage_pips(cfg)
                    slip_px   = slip_pips * ps
                    final_stop = float(effective_stop)   # the stop actually breached this bar (price units)

                    if d > 0:   # long → worse fill below stop
                        exit_px = final_stop - slip_px
                    else:       # short → worse fill above stop
                        exit_px = final_stop + slip_px

                    # Stamp TS audit fields
                    open_tr["ts_active"]        = True
                    open_tr["ts_level"]         = float(final_stop)
                    open_tr["sl_at_exit_price"] = float(final_stop)
                    open_tr["slippage_pips"]    = float(slip_pips)
                # 🔧 ======= END: Trailing-stop exit fill fix =======

                open_tr["exit_date"]   = date_i
                open_tr["exit_price"]  = float(exit_px)
                open_tr["exit_reason"] = reason

                # Classify W/L/S
                if open_tr.get("tp1_hit", False):
                    open_tr["win"] = True; open_tr["loss"] = False; open_tr["scratch"] = False
                else:
                    if reason == "stoploss":
                        open_tr["win"] = False; open_tr["loss"] = True; open_tr["scratch"] = False
                    else:
                        open_tr["win"] = False; open_tr["loss"] = False; open_tr["scratch"] = True

                # Compute PnL (money)
                pip_val_1lot = float(pip_value_per_lot(pair, account_ccy, fx_quotes))
                pnl_money = compute_trade_pnl_money(open_tr, pair, pip_val_1lot)
                open_tr["pnl"] = float(pnl_money)

                # Update global equity and local realized PnL cum
                equity_state["balance"] += float(pnl_money)
                realized_pnl_cum_local  += float(pnl_money)

                # 🔧 Persist the stop that was IN EFFECT at exit (BE/TS/SL aware)
                if reason and "breakeven" in reason:
                    sl_at_exit = open_tr["entry_price"]
                elif reason and ("trailing_stop" in reason or "trailing" in reason):
                    # prefer last ts_level; fallback to current_sl/sl_at_entry if missing
                    sl_at_exit = open_tr.get("ts_level", open_tr.get("current_sl", open_tr.get("sl_at_entry_price", open_tr.get("sl_price"))))
                elif reason and "stop" in reason:
                    sl_at_exit = open_tr.get("current_sl", open_tr.get("sl_at_entry_price", open_tr.get("sl_price")))
                else:
                    sl_at_exit = None  # non-stop exits

                finalized = finalize_trade_row(
                    dict(open_tr),
                    current_stop_price_at_exit=sl_at_exit
                )

                # Append & clear
                trades.append(project_trade_for_csv(finalized))
                open_tr = None

            else:
                # Still open → persist updated fields (do NOT touch the immutable initial SL)
                open_tr["current_sl"] = float(sl_px)
                open_tr["ts_active"]   = bool(ts_active)
                open_tr["ts_level"]    = None if ts_level is None else float(ts_level)
                open_tr["tp1_hit"]     = bool(tp1_done)
                open_tr["breakeven_after_tp1"] = bool(open_tr.get("breakeven_after_tp1", tp1_done))

        # ===========================
        # 2) Equity snapshot per bar
        # ===========================
        if return_equity:
            equity_history.append({
                "date": pd.to_datetime(date_i),
                "pair": pair,
                "pnl_realized_cum": float(realized_pnl_cum_local),
                "equity": float(equity_state["balance"]),
            })

        # ===========================
        # 3) New entry (only if flat)
        # ===========================
        if open_tr is None and entry_sig != 0:
            direction = "long" if entry_sig > 0 else "short"
            d_int = 1 if entry_sig > 0 else -1

            # Use CLOSE for entry (per current spec)
            entry_px  = c_i

            # Guard against invalid ATR at entry
            atr_entry = atr_i
            if (not math.isfinite(atr_entry)) or atr_entry <= 0.0:
                # Skip opening until ATR valid (prevents NaN TP/SL levels & bad sizing)
                continue

            atr_pips  = to_pips(atr_entry)

            sl_dist_pips  = SL_ATR_MULT * atr_pips
            tp1_dist_pips = TP1_ATR_MULT * atr_pips

            # ---------- DBCVIX filter ----------
            trade_date = pd.to_datetime(r["date"]) if "date" in r else pd.to_datetime(rows.loc[i, "date"])
            risk_pct_eff, dbcvix_flag, dbcvix_val = resolve_dbcvix_risk(
                dbcvix_series=dbcvix_series,
                trade_date=trade_date,
                base_risk=risk_pct,
                fcfg=dbcvix_cfg,
            )

            # Optional hard block: skip opening when mode=block triggered
            if risk_pct_eff <= 0.0:
                # mode='block' path only
                continue

            # ---------- Position sizing (USE EFFECTIVE RISK) ----------
            risk_money   = float(equity_state["balance"]) * float(risk_pct_eff)
            pip_val_1lot = float(pip_value_per_lot(pair, account_ccy, fx_quotes))
            lots_total   = 0.0
            if sl_dist_pips > 0 and pip_val_1lot > 0:
                lots_total = risk_money / (sl_dist_pips * pip_val_1lot)
            MIN_LOT = 0.01
            if (not math.isfinite(lots_total)):
                continue
            if lots_total <= 0.0:
                lots_total = MIN_LOT

            lots_half   = lots_total / 2.0
            lots_runner = lots_total - lots_half

            # Price levels (in PRICE units) using ENTRY ATR
            tp1_px = entry_px + d_int * (TP1_ATR_MULT * atr_entry)
            sl_px  = entry_px - d_int * (SL_ATR_MULT * atr_entry)

            # Final sanity on levels
            if not (math.isfinite(tp1_px) and math.isfinite(sl_px)):
                continue

            spread_pips_used = resolve_spread_pips(pair, r, cfg)

            open_tr = {
                "pair": pair,
                "entry_date": date_i,
                "entry_price": float(entry_px),
                "direction": direction,
                "direction_int": int(d_int),

                "atr_at_entry_price": float(atr_entry),
                "atr_at_entry_pips": float(atr_pips),

                "lots_total": float(lots_total),
                "lots_half": float(lots_half),
                "lots_runner": float(lots_runner),

                # --- DBCVIX audit fields ---
                "risk_pct_used": float(risk_pct_eff),
                "dbcvix_val": (float(dbcvix_val) if dbcvix_val is not None else None),
                "dbcvix_flag": bool(dbcvix_flag),

                # --- ENTRY LEVELS (immutable for audit) ---
                "tp1_price": float(tp1_px),
                "sl_price": float(sl_px),
                "tp1_at_entry_price": float(tp1_px),
                "sl_at_entry_price": float(sl_px),

                # --- DYNAMIC STOP STATE (updated by BE/TS logic only) ---
                "current_sl": float(sl_px),
                "ts_active": False,
                "ts_level": None,

                "tp1_hit": False,
                "breakeven_after_tp1": False,

                "entry_idx": int(i),
                "exit_date": None,
                "exit_price": None,
                "exit_reason": None,

                "pnl": 0.0,
                "win": False,
                "loss": False,
                "scratch": False,

                "spread_pips_used": float(spread_pips_used),
            }

    # ---- Return (with optional equity history)
    if return_equity:
        eq_df = pd.DataFrame(equity_history, columns=["date", "pair", "pnl_realized_cum", "equity"])
        try:
            eq_df["date"] = pd.to_datetime(eq_df["date"])
        except Exception:
            pass
        return trades, eq_df

    return trades



# =============================================
# Runner
# =============================================

from typing import Any, Dict, List, Optional, Union

# Global counter for DBCVIX block-mode skips
DBCVIX_BLOCKED_GLOBAL = 0

# =============================================
# Helper: Get the last value at/<= a timestamp
# =============================================
def _get_last_val_at_or_before(series, ts):
    """Return last value at/<= ts or None."""
    try:
        if series is None or getattr(series, "empty", True):
            return None
        sub = series.loc[:ts]
        if sub.empty:
            return None
        return float(sub.iloc[-1])
    except Exception:
        return None


# =============================================
# Canonical DBCVIX resolver (ALWAYS returns 3-tuple)
# =============================================
def resolve_dbcvix_risk(dbcvix_series, trade_date, base_risk, fcfg):
    """Return (risk_pct_eff, dbcvix_flag, dbcvix_val) with risk in DECIMAL units (e.g., 0.02 = 2%).

    Behavior:
      - disabled/no data              -> (base_risk, False, None)
      - val > threshold & mode=block  -> (0.0, True, val)     # caller should skip entry
      - val > threshold & mode!=block -> (min(base_risk, reduce_to), True, val)
      - else                          -> (base_risk, False, val)
    """
    try:
        import pandas as pd
        if not fcfg or not fcfg.get("enabled"):
            return float(base_risk), False, None
        thr = fcfg.get("threshold")
        if thr is None:
            return float(base_risk), False, None
        mode = str(fcfg.get("mode", "reduce"))
        reduce_to = float(fcfg.get("reduce_risk_to", 0.01))  # decimal (1%)
        ts = pd.to_datetime(trade_date)
        val = _get_last_val_at_or_before(dbcvix_series, ts)
        if val is None:
            return float(base_risk), False, None
        if val > float(thr):
            if mode == "block":
                return 0.0, True, val
            return min(float(base_risk), reduce_to), True, val
        return float(base_risk), False, val
    except Exception:
        return float(base_risk), False, None

PathLike = Union[str, Path]

def run_backtest(
    config_path: str | "PathLike[str]" | dict = "config.yaml",
    results_dir: str | "PathLike[str]" | None = None,
):
    """
    Runs the backtest for all configured pairs and writes results to the specified results_dir.
    If results_dir is not passed, falls back to config.yaml -> output.results_dir or 'results'.
    Now also writes an equity curve built from per-pair realized PnL series.

    Accepts either:
      - a path to a YAML config (str / Path-like), or
      - a config dict (already loaded/overridden in code).
    """
    # 1) Load/normalize config
    if isinstance(config_path, dict):
        config = config_path
    else:
        config = load_config(config_path)

    cfg = config  # keep downstream variable name unchanged

    # 2) Resolve results directory
    if results_dir is None:
        results_dir = config.get("output", {}).get("results_dir", "results")
    out_dir = ensure_results_dir(results_dir)

    # ————— DBCVIX: load once and reuse —————
    # near the top of run_backtest(), after you have `config`/`cfg`
    dbcvix_series = load_dbcvix_series(cfg)  # may be None if disabled or missing
    if dbcvix_series is None:
        print("ℹ️  DBCVIX disabled or not loaded (series=None). Risk filter will not trigger.")
    else:
        print(f"ℹ️  DBCVIX loaded: {dbcvix_series.index.min().date()} → {dbcvix_series.index.max().date()} "
              f"(n={len(dbcvix_series)})")

    
    fcfg = ((cfg.get("filters") or {}).get("dbcvix") or {})
    print("ℹ️  DBCVIX config:", {
        "enabled": fcfg.get("enabled"),
        "mode": fcfg.get("mode"),
        "threshold": fcfg.get("threshold"),
        "reduce_risk_to": fcfg.get("reduce_risk_to"),
        "source": fcfg.get("source"),
    })
    
    if dbcvix_series is not None:
        # show a value around a known entry date window to prove lookups work
        sample_dates = [pd.Timestamp("2020-02-05"), pd.Timestamp("2024-10-17")]
        for sd in sample_dates:
            v = dbcvix_series[:sd].tail(1)
            print(f"ℹ️  DBCVIX at/<= {sd.date()}:",
                  None if v.empty else float(v.iloc[0]))
    # Output paths
    trades_path  = out_dir / "trades.csv"
    summary_path = out_dir / "summary.txt"
    equity_path  = out_dir / "equity_curve.csv"

    # 3) Data settings
    pairs    = config.get("pairs") or config.get("data", {}).get("pairs") or []
    data_dir = config.get("data_dir") or config.get("data", {}).get("dir") or "data/daily"

    # 4) Execution settings (kept for reference/future logs)
    entry_cfg = (config.get("entry") or {})
    SL_ATR_MULT     = float(entry_cfg.get("sl_atr", 1.5))
    TP1_ATR_MULT    = float(entry_cfg.get("tp1_atr", 1.0))
    TRAIL_AFTER_ATR = float(entry_cfg.get("trail_after_atr", 2.0))
    TS_ATR_MULT     = float(entry_cfg.get("ts_atr", 1.5))

    # 5) Equity + toggles
    # Prefer risk.starting_balance but fall back to top-level starting_balance, then 10k
    starting_balance = float(
        (config.get("risk") or {}).get("starting_balance",
            config.get("starting_balance", 10_000.0))
    )
    equity_state = {"balance": starting_balance}

    track_equity = (config.get("tracking") or {}).get("in_sim_equity", True)

    # Collectors
    all_trades: list[dict[str, Any]] = []
    equity_frames: list[pd.DataFrame] = []

    # -----------------------------
    # Per-pair pipeline
    # -----------------------------
    for pair in pairs:
        try:
            df = load_pair_csv(pair, data_dir)
            if df is None or df.empty:
                print(f"⚠️ Skipping {pair}: No data found in {data_dir}")
                continue

            # Indicators → signals
            # Indicators → contract validation → signals  (v1.9.6)
            df["pair"] = pair

            # Ensure ATR (your contract requires 'atr')
            base = calculate_atr(df.copy())

            # Cache-aware indicators
            base = apply_indicators_with_cache(base, pair, config)

            # DataFrame contract validation (your original validators_util)
            if (config.get("validation", {}) or {}).get("enabled", True):
                validate_contract(
                    base,
                    config=config,
                    strict=(config.get("validation", {}) or {}).get("strict_contract", False)
                )

            # Generate signals on the validated frame
            signals_df = apply_signal_logic(base, config)
           # --- Normalize signal columns to ints in {-1,0,1} ---
            for col in ["entry_signal", "exit_signal"]:
                if col in signals_df.columns:
                    signals_df[col] = (
                        pd.to_numeric(signals_df[col], errors="coerce")
                          .fillna(0)
                          .clip(-1, 1)
                          .astype(int)
                    )
                else:
                    signals_df[col] = 0
                    
            # Simulate (optionally collecting per-bar realized PnL for this pair)
            if track_equity:
                pair_trades, pair_eq = simulate_pair_trades(
                    rows=signals_df,
                    pair=pair,
                    cfg=config,
                    equity_state=equity_state,
                    return_equity=True,
                    dbcvix_series=dbcvix_series,
                )
                equity_frames.append(pair_eq)
            else:
                pair_trades = simulate_pair_trades(
                    rows=signals_df,
                    pair=pair,
                    cfg=config,
                    equity_state=equity_state,
                    return_equity=False,
                    dbcvix_series=dbcvix_series,
                )

            all_trades.extend(pair_trades)

        except Exception as e:
            print(f"❌ Error processing {pair}: {e}")
            continue

    # -----------------------------
    # Build trades DataFrame
    # -----------------------------
    if all_trades:
        trades_df = pd.DataFrame(all_trades, columns=TRADES_COLS).copy()
    else:
        trades_df = pd.DataFrame(columns=TRADES_COLS)

    # -----------------------------
    # Build total equity curve (from realized PnL only)
    # -----------------------------
    if track_equity and len(equity_frames) > 0:
        eq = pd.concat(equity_frames, ignore_index=True)

        # Wide (one col per pair), last obs per timestamp, forward-fill
        eq_wide = (
            eq.pivot_table(index="date", columns="pair", values="pnl_realized_cum", aggfunc="last")
              .sort_index()
              .ffill()
              .fillna(0.0)
        )

        # Total realized PnL to date across pairs
        eq_wide["pnl_realized_cum_total"] = eq_wide.sum(axis=1, numeric_only=True)

        # Equity = starting_balance + realized PnL (no floating PnL yet)
        equity_curve = eq_wide[["pnl_realized_cum_total"]].rename(
            columns={"pnl_realized_cum_total": "equity"}
        )
        equity_curve["equity"] = starting_balance + equity_curve["equity"]

        # Peak & drawdown (step-like since realized-only)
        equity_curve["peak"] = equity_curve["equity"].cummax()
        equity_curve["drawdown"] = equity_curve["equity"] - equity_curve["peak"]

        equity_curve = equity_curve.reset_index()  # expose 'date'
    else:
        equity_curve = pd.DataFrame(columns=["date", "equity", "peak", "drawdown"])

    # --- Ensure full schema before writing trades.csv ---
    missing = [c for c in TRADES_COLS if c not in trades_df.columns]
    for c in missing:
        trades_df[c] = pd.NA
    trades_df = trades_df.reindex(columns=TRADES_COLS)


    # -----------------------------
    # Writes
    # -----------------------------
    trades_df.to_csv(trades_path, index=False)

    # Summary (pass an extra flag so you can see the equity count at a glance)
    extra_summary = {"equity_curve_rows": int(len(equity_curve))}
    write_summary(summary_path, trades_df, config, extra=extra_summary)

    if track_equity and not equity_curve.empty:
        equity_curve.to_csv(equity_path, index=False)

    print(f"✅ Backtest complete. Results saved to '{out_dir}'")


def run_backtest_walk_forward(config_path: PathLike = "config.yaml",
                              results_dir: Optional[PathLike] = None) -> None:
    """
    Rolling walk-forward: runs OOS-only per fold and aggregates OOS trades across folds.
    Writes under results/<run_name>/:
      - trades.csv (OOS-only trades combined)
      - equity_curve.csv (OOS-only, realized-PnL-based per-bar if enabled; fallback to exit-date steps)
      - oos_summary.txt (metrics across all OOS trades)
      - wfo_folds.csv (per-fold OOS metrics)
    """
    import re
    from pathlib import Path
    from typing import List, Dict, Any
    import pandas as pd
    import numpy as np

    # --- local safe Series helpers (avoid scalar .fillna() errors) ---
    def _num_series(df: pd.DataFrame, col: str, default: float = 0.0) -> pd.Series:
        if col in df.columns:
            return pd.to_numeric(df[col], errors="coerce").fillna(0.0)
        # return a series of defaults matching df length
        return pd.Series([default] * len(df), index=df.index, dtype=float)

    def _bool_series(df: pd.DataFrame, col: str) -> pd.Series:
        if col in df.columns:
            s = pd.to_numeric(df[col], errors="coerce")
            return s.fillna(0).astype(int).astype(bool)
        return pd.Series([False] * len(df), index=df.index, dtype=bool)

    cfg = load_config(config_path)

    wf_cfg = (cfg.get("walk_forward") or {})
    wf_run_name = wf_cfg.get("run_name") or "wfo_default"
    if results_dir is None:
        out_dir = ensure_results_dir(Path("results") / wf_run_name, cfg)
    else:
        out_dir = ensure_results_dir(results_dir, cfg)

    trades_path  = out_dir / "trades.csv"
    summary_path = out_dir / "oos_summary.txt"
    equity_path  = out_dir / "equity_curve.csv"
    folds_csv    = out_dir / "wfo_folds.csv"

    seed = wf_cfg.get("seed")
    if seed is not None:
        try:
            np.random.seed(int(seed))
        except Exception:
            pass

    start = pd.to_datetime(wf_cfg.get("start"))
    end   = pd.to_datetime(wf_cfg.get("end"))
    train_years = int(wf_cfg.get("train_years", 3))
    test_years  = int(wf_cfg.get("test_years", 1))
    step_years  = int(wf_cfg.get("step_years", 1))
    if pd.isna(start) or pd.isna(end):
        raise ValueError("walk_forward.start and walk_forward.end must be set (YYYY-MM-DD).")

    pairs    = cfg.get("pairs") or cfg.get("data", {}).get("pairs") or []
    data_dir = cfg.get("data_dir") or cfg.get("data", {}).get("dir") or "data/daily"
    if not pairs:
        raise ValueError("No pairs configured for walk-forward.")

    starting_balance = float((cfg.get("risk") or {}).get("starting_balance",
                              cfg.get("starting_balance", 10_000.0)))
    track_equity = bool((cfg.get("tracking") or {}).get("in_sim_equity", True))

    # Indicator applier resolution
    _apply_inds = None
    try:
        from indicators_cache import apply_indicators_with_cache as _apply_inds
    except Exception:
        try:
            from backtester_helpers import apply_indicators_with_cache as _apply_inds
        except Exception:
            try:
                from backtester_helpers import apply_indicators as _apply_inds
            except Exception:
                try:
                    from signal_logic import apply_indicators as _apply_inds
                except Exception:
                    _apply_inds = None
    if _apply_inds is None:
        raise RuntimeError("No indicator application function found. Provide apply_indicators_with_cache or apply_indicators.")

    # Pre-load data
    pair_data: Dict[str, pd.DataFrame] = {}
    for p in pairs:
        try:
            df = load_pair_csv(p, data_dir)
        except TypeError:
            df = load_pair_csv(p)
        if df is not None and not df.empty:
            df = df.copy()
            if "date" not in df.columns:
                raise KeyError(f"'date' column missing in data for {p}")
            df["date"] = pd.to_datetime(df["date"])
            df["pair"] = p
            pair_data[p] = df
        else:
            print(f"⚠️  Skipping {p}: No data for WFO")
    if not pair_data:
        raise RuntimeError("No data available for any pairs; cannot run WFO.")

    all_oos_trades: List[Dict[str, Any]] = []
    per_fold_rows: List[Dict[str, Any]] = []

    equity_state: Dict[str, Any] = {"balance": starting_balance}
    fold_equity_frames: List[pd.DataFrame] = []
    realized_so_far = 0.0

    # --- fold loop ---
    fold_idx = 0
    for is_start, is_end, oos_start, oos_end in _date_range_folds(start, end, train_years, test_years, step_years):
        if not (pd.to_datetime(is_end) < pd.to_datetime(oos_start)):
            raise AssertionError(f"No-lookahead violation: train_end {is_end} !< oos_start {oos_start}")

        fold_idx += 1
        print(f"—— Fold {fold_idx} ———————————————————————————————")
        print(f"IS:  {pd.to_datetime(is_start).date()} → {pd.to_datetime(is_end).date()}")
        print(f"OOS: {pd.to_datetime(oos_start).date()} → {pd.to_datetime(oos_end).date()}")

        fold_trades: List[Dict[str, Any]] = []
        pair_eq_frames: List[pd.DataFrame] = []

        for pair, df in pair_data.items():
            try:
                base = df.copy()

                from utils import calculate_atr
                base = calculate_atr(base)

                base = _apply_inds(base, pair, cfg) if _apply_inds.__code__.co_argcount >= 3 else _apply_inds(base, cfg)

                try:
                    from validators_util import validate_contract
                    if (cfg.get("validation", {}) or {}).get("enabled", True):
                        validate_contract(
                            base,
                            config=cfg,
                            strict=(cfg.get("validation", {}) or {}).get("strict_contract", False)
                        )
                except Exception as _ve:
                    print(f"ℹ️  {pair}: validation skipped/failed ({_ve})")

                base = apply_signal_logic(base, cfg)

                for col in ["entry_signal", "exit_signal"]:
                    if col in base.columns:
                        base[col] = pd.to_numeric(base[col], errors="coerce").fillna(0).clip(-1, 1).astype(int)
                    else:
                        base[col] = 0

                oos_rows = _slice_df_by_dates(base, oos_start, oos_end)
                if oos_rows.empty:
                    continue

                if track_equity:
                    sim_out = simulate_pair_trades(
                        oos_rows, pair, cfg, equity_state,
                        return_equity=True
                    )
                    if sim_out:
                        trades, pair_eq = sim_out
                        if trades:
                            fold_trades.extend(trades)
                        if pair_eq is not None and not pair_eq.empty:
                            pair_eq = pair_eq[["date", "pair", "pnl_realized_cum"]].copy()
                            pair_eq["fold"] = fold_idx
                            pair_eq_frames.append(pair_eq)
                else:
                    sim_out = simulate_pair_trades(
                        oos_rows, pair, cfg, equity_state,
                        return_equity=False
                    )
                    if sim_out:
                        fold_trades.extend(sim_out)

            except Exception as e:
                print(f"❌ WFO fold {fold_idx} {pair}: {e}")

        # ----- Fold metrics -----
        fold_df = pd.DataFrame(fold_trades)

        fold_roi_pct = None
        fold_maxdd_pct = None
        if track_equity and pair_eq_frames:
            eq = pd.concat(pair_eq_frames, ignore_index=True)
            eq_wide = (
                eq.pivot_table(index="date", columns="pair", values="pnl_realized_cum", aggfunc="last")
                  .sort_index().ffill().fillna(0.0)
            )
            eq_wide["pnl_realized_cum_total"] = eq_wide.sum(axis=1, numeric_only=True)
            eq_fold_equity = (starting_balance + eq_wide["pnl_realized_cum_total"]).astype(float)
            if len(eq_fold_equity) >= 2:
                fold_roi_pct = float((eq_fold_equity.iloc[-1] / eq_fold_equity.iloc[0] - 1.0) * 100.0)
                rolling_peak = eq_fold_equity.cummax()
                dd = (eq_fold_equity / rolling_peak - 1.0) * 100.0
                fold_maxdd_pct = float(dd.min())
        else:
            pnl_sum = float(_num_series(fold_df, "pnl", 0.0).sum())
            fold_roi_pct = float((pnl_sum / starting_balance) * 100.0) if starting_balance else 0.0
            fold_maxdd_pct = None

        total = int(len(fold_df))
        wins = int(_bool_series(fold_df, "win").sum()) if total else 0
        losses = int(_bool_series(fold_df, "loss").sum()) if total else 0
        scratches = int(_bool_series(fold_df, "scratch").sum()) if total else 0
        ns = max(wins + losses, 0)

        per_fold_rows.append({
            "fold": fold_idx,
            "is_start": pd.to_datetime(is_start).date(),
            "is_end": pd.to_datetime(is_end).date(),
            "oos_start": pd.to_datetime(oos_start).date(),
            "oos_end": pd.to_datetime(oos_end).date(),
            "oos_trades": total,
            "win_pct_ns": (wins / ns * 100.0) if ns else 0.0,
            "oos_roi_pct": fold_roi_pct,
            "oos_max_dd_pct": fold_maxdd_pct,
        })

        if total:
            all_oos_trades.extend(fold_trades)

        if track_equity and pair_eq_frames:
            eq = pd.concat(pair_eq_frames, ignore_index=True)
            eq_wide = (
                eq.pivot_table(index="date", columns="pair", values="pnl_realized_cum", aggfunc="last")
                  .sort_index().ffill().fillna(0.0)
            )
            eq_wide["pnl_realized_cum_total"] = eq_wide.sum(axis=1, numeric_only=True)
            fold_equity = eq_wide[["pnl_realized_cum_total"]].rename(columns={"pnl_realized_cum_total": "equity"})
            fold_equity["equity"] = (starting_balance + realized_so_far) + fold_equity["equity"]
            fold_equity = fold_equity.reset_index()
            fold_equity["fold"] = fold_idx
            fold_equity_frames.append(fold_equity)

            realized_so_far += float(_num_series(fold_df, "pnl", 0.0).sum())

    # ----- Write combined OOS trades -----
    oos_df = pd.DataFrame(all_oos_trades)

    try:
        from utils import TRADES_COLS
    except Exception:
        TRADES_COLS = list(oos_df.columns)

    if oos_df.empty:
        for c in TRADES_COLS:
            if c not in oos_df.columns:
                oos_df[c] = pd.Series(dtype="object")
        if "exit_reason" not in oos_df.columns:
            oos_df["exit_reason"] = pd.Series(dtype="object")
        oos_df = oos_df[TRADES_COLS]
    else:
        for c in TRADES_COLS:
            if c not in oos_df.columns:
                oos_df[c] = pd.NA
        oos_df = oos_df[TRADES_COLS]

    oos_df.to_csv(trades_path, index=False)
    print(f"✅ Wrote OOS trades: {trades_path}")

    # ----- Per-fold CSV -----
    if per_fold_rows:
        pd.DataFrame(per_fold_rows).to_csv(folds_csv, index=False)
        print(f"✅ Wrote folds CSV: {folds_csv}")

    # ----- Build final OOS equity curve -----
    if track_equity and fold_equity_frames:
        equity_df = pd.concat(fold_equity_frames, ignore_index=True).sort_values("date")
        equity_df["peak"] = equity_df["equity"].cummax()
        equity_df["drawdown"] = equity_df["equity"] - equity_df["peak"]
        equity_df = equity_df[["date", "equity", "peak", "drawdown"]]
        equity_df.to_csv(equity_path, index=False)
        print(f"✅ Wrote OOS equity: {equity_path}")
    else:
        equity_df = pd.DataFrame()
        try:
            pnl_series = _num_series(oos_df, "pnl", 0.0)
            dates = pd.to_datetime(oos_df["exit_date"], errors="coerce") if "exit_date" in oos_df.columns else pd.Series(pd.NaT, index=oos_df.index)
            equity_vals = pnl_series.cumsum() + starting_balance
            equity_df   = pd.DataFrame({"date": dates, "equity": equity_vals}).dropna(subset=["date"])
            equity_df   = equity_df.sort_values("date").reset_index(drop=True)
            equity_df["peak"] = equity_df["equity"].cummax()
            equity_df["drawdown"] = equity_df["equity"] - equity_df["peak"]
            if not equity_df.empty:
                equity_df.to_csv(equity_path, index=False)
                print(f"✅ Wrote OOS equity (fallback): {equity_path}")
        except Exception as e:
            print(f"ℹ️  Equity fallback failed: {e}")

    # ----- OOS Summary -----
    try:
        from utils import summarize_results
        txt = summarize_results(results_dir=str(out_dir), config_path=str(config_path))
    except Exception:
        total = len(oos_df)
        wins = int(_bool_series(oos_df, "win").sum()) if total else 0
        losses = int(_bool_series(oos_df, "loss").sum()) if total else 0
        scratches = int(_bool_series(oos_df, "scratch").sum()) if total else 0
        non_scratch = max(wins + losses, 0)
        roi_dollars = float(_num_series(oos_df, "pnl", 0.0).sum())
        roi_pct = (roi_dollars / starting_balance * 100.0) if starting_balance else 0.0
        txt = (
            f"📊 WFO OOS Summary\n"
            f"-------------------\n"
            f"Total Trades : {total}\n"
            f"Wins         : {wins}\n"
            f"Losses       : {losses}\n"
            f"Scratches    : {scratches}\n"
            f"Win% (non-scratch) : {(wins/non_scratch*100.0) if non_scratch else 0.0:.2f}\n"
            f"Loss% (non-scratch): {(losses/non_scratch*100.0) if non_scratch else 0.0:.2f}\n"
            f"ROI ($)      : {roi_dollars:.2f}\n"
            f"ROI (%)      : {roi_pct:.2f}\n"
        )
    # ----- Optional: auto-run Monte Carlo after WFO -----
    try:
        mc_cfg = (cfg.get("monte_carlo") or {})
        if mc_cfg.get("enabled") and mc_cfg.get("auto_after_wfo", False):
            print("▶️  Auto-running Monte Carlo…")
            try:
                from analytics.monte_carlo import run_monte_carlo as _run_mc
                eq_df = pd.read_csv(equity_path) if equity_path.exists() else None
                td_df = pd.read_csv(trades_path) if trades_path.exists() else None
                # ✅ Force MC to write into THIS run's folder
                _ = _run_mc(cfg, equity_df=eq_df, trades_df=td_df, results_dir_override=out_dir)
                print("✅ Monte Carlo complete (auto)")
            except Exception as e:
                print(f"⚠️ Monte Carlo auto run skipped: {e}")
    except Exception:
        pass

    summary_path.write_text(txt, encoding="utf-8")
    print(f"✅ Walk-forward complete. OOS results saved to '{out_dir}'")


# ---------------- DBCVIX helpers (v1.9.7) ----------------
def _load_dbcvix_series(cfg):
    try:
        f = ((cfg or {}).get("filters") or {}).get("dbcvix") or {}
        if not f.get("enabled"):
            return None, f
        import pandas as pd
        path = f.get("csv_path") or "data/external/dbcvix_synth.csv"
        col  = f.get("column")   or "cvix_synth"
        df = pd.read_csv(path)
        # accept either a 'date' column or first column as date
        date_col = "date" if "date" in df.columns else df.columns[0]
        s = pd.to_datetime(df[date_col])
        v = pd.to_numeric(df[col], errors="coerce")
        ser = v.set_axis(s).sort_index()
        return ser, f
    except Exception:
        return None, ((cfg or {}).get("filters") or {}).get("dbcvix") or {}

def _dbcvix_value_asof(ser, when_ts):
    if ser is None or ser.empty:
        return None
    # take last value at/<= timestamp
    loc = ser.index.searchsorted(when_ts, side="right") - 1
    if loc < 0:
        return None
    return float(ser.iloc[loc])

# ===== Canonical DBCVIX resolver (ALWAYS returns 3-tuple) =====
# ===== Canonical DBCVIX resolver (ALWAYS returns 3-tuple) =====
